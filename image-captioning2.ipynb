{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Captioning System:\n### The systems performs the following:\n1. Using pre-trained CNNs (ResNet-50) for feature extraction from images.\n2. Creating an RNN architecture model which takes the extracted feature as the first hidden state and trains on NTP (Next Token Prediction) task using the cross entropy loss","metadata":{}},{"cell_type":"code","source":"#installing required dependencies\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport cv2\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision.models.feature_extraction import create_feature_extractor\nfrom torchvision.io import read_image\nimport os\nimport re\nfrom collections import Counter\nfrom typing import List, Tuple, Dict, Optional, Any\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtext.vocab import Vocab, vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T19:48:17.499799Z","iopub.execute_input":"2024-11-28T19:48:17.500445Z","iopub.status.idle":"2024-11-28T19:48:17.813955Z","shell.execute_reply.started":"2024-11-28T19:48:17.500406Z","shell.execute_reply":"2024-11-28T19:48:17.812490Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. Feature Extraction using ResNet-50 pretrained model","metadata":{}},{"cell_type":"code","source":"# Function for extracting features using a preloaded model\ndef extract_feature(image_path, feature_extractor, preprocess):\n    # Load and preprocess the image\n    image = read_image(image_path).unsqueeze(0)  # Add batch dimension (1, ...)\n    transformed_image = preprocess(image)\n    \n    # Extract features\n    with torch.no_grad():  # Disable gradient computation for efficiency\n        features = feature_extractor(transformed_image)\n    feature_vector = features['avgpool'].squeeze()\n    return feature_vector.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T19:53:40.140208Z","iopub.execute_input":"2024-11-28T19:53:40.140686Z","iopub.status.idle":"2024-11-28T19:53:40.147810Z","shell.execute_reply.started":"2024-11-28T19:53:40.140647Z","shell.execute_reply":"2024-11-28T19:53:40.146343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_image_paths(base_path):\n    \"\"\"\n    Walks through the base directory and returns a list of image file paths.\n    \"\"\"\n    image_paths = []\n    for root, dirs, files in os.walk(base_path):\n        for file in files:\n            if file.endswith(('.jpg', '.jpeg', '.png')):\n                image_paths.append(os.path.join(root, file))\n    return image_paths\n\ndef extract_features_from_dataset(image_paths, output_file=\"features.npy\", batch_size=32):\n    # Load the model and preprocessing transform only once\n    model_weights = ResNet50_Weights.DEFAULT\n    model = resnet50(weights=model_weights)\n    model.eval()\n    preprocess = model_weights.transforms()\n    \n    return_nodes = {'avgpool': 'avgpool'}\n    feature_extractor = create_feature_extractor(model, return_nodes=return_nodes)\n    \n    # Process images in batches and save features incrementally\n    features = []\n    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Extracting feature vectors\"):\n        batch_paths = image_paths[i:i + batch_size]\n        batch_features = [\n            extract_feature(image_path, feature_extractor, preprocess) for image_path in batch_paths\n        ]\n        features.extend(batch_features)\n\n        # Save to disk after every batch to avoid memory overflow\n        np.save(output_file, np.array(features))\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T19:53:46.126754Z","iopub.execute_input":"2024-11-28T19:53:46.127199Z","iopub.status.idle":"2024-11-28T19:53:46.137160Z","shell.execute_reply.started":"2024-11-28T19:53:46.127164Z","shell.execute_reply":"2024-11-28T19:53:46.135806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_paths = get_image_paths(\"/kaggle/input/flickr8k/Images\")\nfeature_vectors = extract_features_from_dataset(image_paths, output_file=\"/kaggle/working/features.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T19:53:50.920671Z","iopub.execute_input":"2024-11-28T19:53:50.921147Z","iopub.status.idle":"2024-11-28T20:13:48.591266Z","shell.execute_reply.started":"2024-11-28T19:53:50.921109Z","shell.execute_reply":"2024-11-28T20:13:48.589889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#sentences format\nsentence = List[str]\n\n#Function for extracting captions\ndef extract_captions(datapath) -> Tuple[List[sentence], Dict[str, int]]:\n\n    sentences: List[sentence] = []\n    word_cnt: Dict[str,int] = Counter()\n\n    for sentence_txt in open(datapath).read.split(\"\\n\"): #each (img, sentence) pair is in a different line\n        _, sentence = sentence_txt.split(\",\") \n        sentences.append([])\n        for w in sentence.split(\" \"):\n            w = w.lower() #remove uppercase letters\n            w = re.sub(r\"\\W+\", \"\", w) #remove special characters\n            \n            word_cnt[w] += 1\n            sentences[-1].append(w)\n\n\n    return sentences, wrd_cnt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class image_caption_dataset(Dataset):\n\n    def __init__(datapath: str, word_vocab: Optional[Vocab] = None, feature_vectors) -> None:\n\n        self.sentences: List[sentence] = None\n        PAD = \"\"\n        UNKNOWN = \"\"\n\n        self.sentences, word_cnt = extract_captions(datapath)\n        self.feature_vectors = feature_vectors\n\n        if word_vocab = None:\n        word_vocab = vocab(word_cnt, specials = [PAD, UNKNOWN])\n        word_vocab.set_default_index(word_vocab[UNKNOWN]) \n\n        self.word_vocab = word_vocab\n        self.unknown_idx = self.words_vocab[UNKNOWN]\n        self.pad_idx = self.words_vocab[PAD]\n\n    def __getitem__(self, idx: int) -> Sentence:\n        \"\"\"\n        Get the idx'th sentence in the dataset.\n        \"\"\"\n        return self.sentences[idx], self.feature_vectors[idx]\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the number of sentences in the dataset.\n        \"\"\"\n        return len(self.sentences)\n\n    def form_batch(self, sentences: List[sentence], feature_vectors) --> Dict[str, Any]:\n        \n        word: List[List[str]] = []\n        max_len = -1\n        for sent in sentences:\n            word.append([])\n            for w in sent:\n                word[-1].append(w)\n            max_len = max(max_len, len(word[-1]))  \n\n        batch_size  = len(sentences)\n\n        #now we need to fill word_idxs, valid_masks and feature vectors tensor\n        word_idxs = torch.full((batch_size, max_len), fill_value= self.pad_idx, dtype= torch.int640)\n        valid_mask =torch.zeros_like(word_idxs, dtype= torch.bool)\n        feature_vectors = torch.full(())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}